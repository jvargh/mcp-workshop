"use strict";(self.webpackChunkmcp_workshop=self.webpackChunkmcp_workshop||[]).push([[2887],{426:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"mcp-concepts/consuming-server/activity-llm-client","title":"Activity: Create an MCP client using LLM","description":"Let\'s add an LLM and ensure the LLM is what\'s between the user and the server. Here\'s how it will work on high level:","source":"@site/docs/mcp-concepts/01-consuming-server/06-activity-llm-client.md","sourceDirName":"mcp-concepts/01-consuming-server","slug":"/mcp-concepts/consuming-server/activity-llm-client","permalink":"/mcp-workshop/docs/mcp-concepts/consuming-server/activity-llm-client","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/mcp-concepts/01-consuming-server/06-activity-llm-client.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"LLM Client","permalink":"/mcp-workshop/docs/mcp-concepts/consuming-server/llm-client"},"next":{"title":"SSE Servers","permalink":"/mcp-workshop/docs/category/sse-servers"}}');var s=t(4848),o=t(8453);const i={sidebar_position:6},l="Activity: Create an MCP client using LLM",c={},a=[{value:"-1- Create a client file",id:"-1--create-a-client-file",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"activity-create-an-mcp-client-using-llm",children:"Activity: Create an MCP client using LLM"})}),"\n",(0,s.jsx)(n.p,{children:"Let's add an LLM and ensure the LLM is what's between the user and the server. Here's how it will work on high level:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"The client will call the server and get information about its tools. Based on this response, the client will remember the tools and their parameters."}),"\n",(0,s.jsx)(n.li,{children:"The user then asks the client a question which in turn will call the LLM to get a response. If the LLM deems it necessary, it will call the server's tools to get more information."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Next, let's implement this in code."}),"\n",(0,s.jsx)(n.h2,{id:"-1--create-a-client-file",children:"-1- Create a client file"}),"\n",(0,s.jsxs)(n.p,{children:["Use your existing Node.js project but add a ",(0,s.jsx)(n.code,{children:"Client.ts"})," file."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add the following code to ",(0,s.jsx)(n.code,{children:"Client.ts"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { Client } from "@modelcontextprotocol/sdk/client/index.js";\r\nimport { Transport } from "@modelcontextprotocol/sdk/shared/transport";\r\nimport OpenAI from "openai";\r\nimport { z } from "zod"; // Import zod for schema validation\r\n\r\nthis.openai = new OpenAI({\r\n    baseURL: "https://models.inference.ai.azure.com", // might need to change to this url in the future: https://models.github.ai/inference\r\n    apiKey: process.env.GITHUB_TOKEN,\r\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now you have all the imports you need including OpenAI and you've instantiated an openai client. Next, let's add code to create the client and connect to the server."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add the following code to create a client and connect to the server:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'async function main() {\r\n    const transport = new StdioClientTransport({\r\n        command: "node",\r\n        args: ["../build/index.js"]\r\n    });\r\n\r\n    const client = new Client(\r\n    {\r\n        name: "example-client",\r\n        version: "1.0.0"\r\n    },\r\n    {\r\n        capabilities: {\r\n        prompts: {},\r\n        resources: {},\r\n        tools: {}\r\n        }\r\n    }\r\n    );\r\n\r\n    await client.connect(transport);\r\n}\r\n\r\nmain().catch((error) => {\r\n    console.error("Fatal error: ", error);\r\n    process.exit(1);\r\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now you have a client, let's add the LLM part next."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add the following code to help transform a server response to a tool description:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'function openAiToolAdapter(tool: {\r\n  name: string;\r\n  description?: string;\r\n  input_schema: any;\r\n    }) {\r\n    // Create a zod schema based on the input_schema\r\n    const schema = z.object(tool.input_schema);\r\n\r\n    return {\r\n    type: "function",\r\n    function: {\r\n        name: tool.name,\r\n        description: tool.description,\r\n        parameters: {\r\n        type: "object",\r\n        properties: tool.input_schema.properties,\r\n        required: tool.input_schema.required,\r\n        },\r\n    },\r\n    };\r\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"This function takes a tool description from the server and transforms it into a format that OpenAI can understand. Our next step is to make a call to the server and get the tools."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add the following code to ",(0,s.jsx)(n.code,{children:"main()"})," to list the tools from the server:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"const tools = await this.mcp.listTools();\r\n  this.tools = toolsResult.tools.map((tool) => {\r\n    return openAiToolAdapter({\r\n      name: tool.name,\r\n      description: tool.description,\r\n      input_schema: tool.inputSchema,\r\n    });\r\n});\n"})}),"\n",(0,s.jsx)(n.p,{children:"This sets us up nicely to call the LLM. Next, let's add the code to call the LLM and get a response and based on that response, determine if we need to call the server's tools."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add the following to make the call to the LLM:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'let response = await this.openai.chat.completions.create({\r\n  model: "gpt-4o-mini",\r\n  max_tokens: 1000,\r\n  messages,\r\n  tools: tools,\r\n});\r\n\n'})}),"\n",(0,s.jsx)(n.p,{children:"Once we've got the response, we need to check if the LLM has called any of the tools. If it has, we need to call the server's tools and get the result."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add this code to inspect the LLM response:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'results = any[];\r\n\r\nasync callTools(\r\n    tool_calls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[],\r\n    toolResults: any[],\r\n    finalText: string[]\r\n  ) {\r\n    for (const tool_call of tool_calls) {\r\n      const toolName = tool_call.function.name;\r\n      const args = tool_call.function.arguments;\r\n\r\n      console.log(`Calling tool ${toolName} with args ${JSON.stringify(args)}`);\r\n\r\n\r\n      // 2. Call the server\'s tool \r\n      const toolResult = await this.mcp.callTool({\r\n        name: toolName,\r\n        arguments: JSON.parse(args),\r\n      });\r\n\r\n      console.log("Tool result: ", toolResult);\r\n\r\n      // 3. Do something with the result\r\n      // TODO  \r\n\r\n     }\r\n}\r\n\r\n// 1. Go through the LLM response,for each choice, check if it has tool calls \r\nresponse.choices.map(async (choice) => {\r\n  const message = choice.message;\r\n  if (message.tool_calls) {\r\n    toolResults.push(\r\n      await callTools(message.tool_calls, results, finalText)\r\n    );\r\n  } else {\r\n    console.log("Message response from LLM: ", message);\r\n    finalText.push(message.content || "xx");\r\n  }\r\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now we have a client with the capability to call the server's tools based on the LLM response. Let's test this code out next."}),"\n"]}),"\n",(0,s.jsx)(n.li,{}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var r=t(6540);const s={},o=r.createContext(s);function i(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);