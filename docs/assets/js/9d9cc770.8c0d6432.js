"use strict";(self.webpackChunkmcp_workshop=self.webpackChunkmcp_workshop||[]).push([[2887],{426:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>h});const o=JSON.parse('{"id":"mcp-concepts/consuming-server/activity-llm-client","title":"Activity: Create an MCP client using LLM","description":"Let\'s add an LLM and ensure the LLM is what\'s between the user and the server. Here\'s how it will work on high level:","source":"@site/docs/mcp-concepts/01-consuming-server/06-activity-llm-client.md","sourceDirName":"mcp-concepts/01-consuming-server","slug":"/mcp-concepts/consuming-server/activity-llm-client","permalink":"/mcp-workshop/docs/mcp-concepts/consuming-server/activity-llm-client","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/mcp-concepts/01-consuming-server/06-activity-llm-client.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"LLM Client","permalink":"/mcp-workshop/docs/mcp-concepts/consuming-server/llm-client"},"next":{"title":"SSE Servers","permalink":"/mcp-workshop/docs/category/sse-servers"}}');var s=t(4848),l=t(8453),r=t(5537),a=t(9329);const i={sidebar_position:6},c="Activity: Create an MCP client using LLM",d={},h=[{value:"-1- Create the LLM client",id:"-1--create-the-llm-client",level:2},{value:"-2- Run the client",id:"-2--run-the-client",level:2},{value:"Summary",id:"summary",level:2}];function u(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"activity-create-an-mcp-client-using-llm",children:"Activity: Create an MCP client using LLM"})}),"\n",(0,s.jsx)(n.p,{children:"Let's add an LLM and ensure the LLM is what's between the user and the server. Here's how it will work on high level:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"The client will call the server and get information about its tools. Based on this response, the client will remember the tools and their parameters."}),"\n",(0,s.jsx)(n.li,{children:"The user then asks the client a question which in turn will call the LLM to get a response. If the LLM deems it necessary, it will call the server's tools to get more information."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Next, let's implement this in code."}),"\n",(0,s.jsx)(n.h2,{id:"-1--create-the-llm-client",children:"-1- Create the LLM client"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(a.A,{value:"typescript",children:[(0,s.jsxs)(n.p,{children:["Use your existing Node.js project but add a ",(0,s.jsx)(n.code,{children:"Client.ts"})," file."]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add the following code to ",(0,s.jsx)(n.code,{children:"Client.ts"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { Client } from "@modelcontextprotocol/sdk/client/index.js";\nimport { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";\nimport { Transport } from "@modelcontextprotocol/sdk/shared/transport.js";\nimport OpenAI from "openai";\nimport { z } from "zod"; // Import zod for schema validation\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now you have all the imports you need including OpenAI and you've instantiated an openai client. Next, let's add code to create the client and connect to the server."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add the following code to create a client class:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'class MCPClient {\n    private openai: OpenAI;\n    private client: Client;\n    constructor(){\n        this.openai = new OpenAI({\n            baseURL: "https://models.inference.ai.azure.com", \n            apiKey: process.env.GITHUB_TOKEN,\n        });\n\n        this.client = new Client(\n            {\n                name: "example-client",\n                version: "1.0.0"\n            },\n            {\n                capabilities: {\n                prompts: {},\n                resources: {},\n                tools: {}\n                }\n            }\n            );    \n    }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"In the preceding code:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"MCPClient"})," is defined as a class that will handle the connection to the server and the LLM."]}),"\n",(0,s.jsx)(n.li,{children:"A constructor is defined that initializes the OpenAI client and the MCP client. The OpenAI client is initialized with the base URL and API key. The MCP client is initialized with the name and version."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Great, we now have a client that can connect to the server. Next, let's add more code to wire up the LLM and the server."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add the following so we can connect to the server:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'async connectToServer(transport: Transport) {\n     await this.client.connect(transport);\n     this.run();\n     console.error("MCPClient started on stdin/stdout");\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"It doesn't do much other than connect to the server and run the client."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Let's add the following code to help us convert an MCP tool response into a tool an LLM can understand:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'openAiToolAdapter(tool: {\n     name: string;\n     description?: string;\n     input_schema: any;\n       }) {\n       // Create a zod schema based on the input_schema\n       const schema = z.object(tool.input_schema);\n   \n       return {\n         type: "function" as const, // Explicitly set type to "function"\n         function: {\n           name: tool.name,\n           description: tool.description,\n           parameters: {\n           type: "object",\n           properties: tool.input_schema.properties,\n           required: tool.input_schema.required,\n           },\n         },\n       };\n }\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Next, let's add a function that lets us process a response from an LLM and make a tool call if the LLM indicates a tool should be called:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'async callTools(\n    tool_calls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[],\n    toolResults: any[]\n  ) {\n    for (const tool_call of tool_calls) {\n      const toolName = tool_call.function.name;\n      const args = tool_call.function.arguments;\n\n      console.log(`Calling tool ${toolName} with args ${JSON.stringify(args)}`);\n\n\n      // 2. Call the server\'s tool \n      const toolResult = await this.client.callTool({\n        name: toolName,\n        arguments: JSON.parse(args),\n      });\n\n      console.log("Tool result: ", toolResult);\n\n      // 3. Do something with the result\n      // TODO  \n\n     }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"In the preceding code:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"callTools"})," function takes an array of tool calls and their results. It iterates over the tool calls, calling each tool on the server and logging the result."]}),"\n",(0,s.jsxs)(n.li,{children:["See the part where we call ",(0,s.jsx)(n.code,{children:"callTool"})," with a tool name and arguments. This is how we call a tool on the server. The result is then logged to the console."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Finally, we add the function that sets everything up called ",(0,s.jsx)(n.code,{children:"run"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'async run() {\n    console.log("Asking server for available tools");\n    const toolsResult = await this.client.listTools();\n    const tools = toolsResult.tools.map((tool) => {\n        return this.openAiToolAdapter({\n          name: tool.name,\n          description: tool.description,\n          input_schema: tool.inputSchema,\n        });\n    });\n\n    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [\n        {\n            role: "user",\n            content: "What is the sum of 2 and 3?",\n        },\n    ];\n\n    console.log("Querying LLM: ", messages[0].content);\n    let response = this.openai.chat.completions.create({\n        model: "gpt-4o-mini",\n        max_tokens: 1000,\n        messages,\n        tools: tools,\n    });    \n\n    let results: any[] = [];\n\n    // 1. Go through the LLM response,for each choice, check if it has tool calls \n    (await response).choices.map(async (choice: { message: any; }) => {\n      const message = choice.message;\n      if (message.tool_calls) {\n          console.log("Making tool call")\n          await this.callTools(message.tool_calls, results);\n      }\n    });\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"In the preceding code:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"run"})," function first queries the server for available tools and converts them into a format that the LLM can understand."]}),"\n",(0,s.jsx)(n.li,{children:"It then sends a message to the LLM asking for the sum of 2 and 3."}),"\n",(0,s.jsxs)(n.li,{children:["If the LLM indicates that a tool should be called, it calls the ",(0,s.jsx)(n.code,{children:"callTools"})," function to handle the tool call."]}),"\n"]}),"\n"]}),"\n"]}),(0,s.jsx)(n.p,{children:"Here's the full code for reference:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { Client } from "@modelcontextprotocol/sdk/client/index.js";\nimport { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";\nimport { Transport } from "@modelcontextprotocol/sdk/shared/transport.js";\nimport OpenAI from "openai";\nimport { z } from "zod"; // Import zod for schema validation\n\nclass MyClient {\n    private openai: OpenAI;\n    private client: Client;\n    constructor(){\n        this.openai = new OpenAI({\n            baseURL: "https://models.inference.ai.azure.com", // might need to change to this url in the future: https://models.github.ai/inference\n            apiKey: process.env.GITHUB_TOKEN,\n        });\n\n       \n        \n        this.client = new Client(\n            {\n                name: "example-client",\n                version: "1.0.0"\n            },\n            {\n                capabilities: {\n                prompts: {},\n                resources: {},\n                tools: {}\n                }\n            }\n            );    \n    }\n\n    async connectToServer(transport: Transport) {\n        await this.client.connect(transport);\n        this.run();\n        console.error("MCPClient started on stdin/stdout");\n    }\n\n    openAiToolAdapter(tool: {\n        name: string;\n        description?: string;\n        input_schema: any;\n          }) {\n          // Create a zod schema based on the input_schema\n          const schema = z.object(tool.input_schema);\n      \n          return {\n            type: "function" as const, // Explicitly set type to "function"\n            function: {\n              name: tool.name,\n              description: tool.description,\n              parameters: {\n              type: "object",\n              properties: tool.input_schema.properties,\n              required: tool.input_schema.required,\n              },\n            },\n          };\n    }\n    \n    async callTools(\n        tool_calls: OpenAI.Chat.Completions.ChatCompletionMessageToolCall[],\n        toolResults: any[]\n      ) {\n        for (const tool_call of tool_calls) {\n          const toolName = tool_call.function.name;\n          const args = tool_call.function.arguments;\n    \n          console.log(`Calling tool ${toolName} with args ${JSON.stringify(args)}`);\n    \n    \n          // 2. Call the server\'s tool \n          const toolResult = await this.client.callTool({\n            name: toolName,\n            arguments: JSON.parse(args),\n          });\n    \n          console.log("Tool result: ", toolResult);\n    \n          // 3. Do something with the result\n          // TODO  \n    \n         }\n    }\n\n    async run() {\n        console.log("Asking server for available tools");\n        const toolsResult = await this.client.listTools();\n        const tools = toolsResult.tools.map((tool) => {\n            return this.openAiToolAdapter({\n              name: tool.name,\n              description: tool.description,\n              input_schema: tool.inputSchema,\n            });\n        });\n    \n        const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [\n            {\n                role: "user",\n                content: "What is the sum of 2 and 3?",\n            },\n        ];\n\n        console.log("Querying LLM: ", messages[0].content);\n        let response = this.openai.chat.completions.create({\n            model: "gpt-4o-mini",\n            max_tokens: 1000,\n            messages,\n            tools: tools,\n        });    \n\n        let results: any[] = [];\n    \n        // 1. Go through the LLM response,for each choice, check if it has tool calls \n        (await response).choices.map(async (choice: { message: any; }) => {\n          const message = choice.message;\n          if (message.tool_calls) {\n              console.log("Making tool call")\n              await this.callTools(message.tool_calls, results);\n          }\n        });\n    }\n    \n}\n\nlet client = new MyClient();\n const transport = new StdioClientTransport({\n            command: "node",\n            args: ["./build/index.js"]\n        });\n\nclient.connectToServer(transport);\n'})}),(0,s.jsx)(n.p,{children:"That's it, now you have a working client."})]}),(0,s.jsxs)(a.A,{value:"python",label:"Python",default:!0,children:[(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create a file ",(0,s.jsx)(n.em,{children:"client.py"})," and add the following code:"]}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\n\n# llm\nimport os\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import SystemMessage, UserMessage\nfrom azure.core.credentials import AzureKeyCredential\nimport json\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command="mcp",  # Executable\n    args=["run", "server.py"],  # Optional command line arguments\n    env=None,  # Optional environment variables\n)\n'})}),(0,s.jsx)(n.p,{children:"Now you have the needed imports and you've created some basic stdio parameters that will run the server once the client is started."}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Add the following helper functions:"}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def call_llm(prompt, functions):\n  token = os.environ["GITHUB_TOKEN"]\n  endpoint = "https://models.inference.ai.azure.com"\n\n  model_name = "gpt-4o"\n\n  client = ChatCompletionsClient(\n      endpoint=endpoint,\n      credential=AzureKeyCredential(token),\n  )\n\n  print("CALLING LLM")\n  response = client.complete(\n      messages=[\n          {\n          "role": "system",\n          "content": "You are a helpful assistant.",\n          },\n          {\n          "role": "user",\n          "content": prompt,\n          },\n      ],\n      model=model_name,\n      tools = functions,\n      # Optional parameters\n      temperature=1.,\n      max_tokens=1000,\n      top_p=1.    \n  )\n\n  response_message = response.choices[0].message\n  \n  functions_to_call = []\n\n  if response_message.tool_calls:\n      for tool_call in response_message.tool_calls:\n          print("TOOL: ", tool_call)\n          name = tool_call.function.name\n          args = json.loads(tool_call.function.arguments)\n          functions_to_call.append({ "name": name, "args": args })\n\n  return functions_to_call\n\ndef convert_to_llm_tool(tool):\n    tool_schema = {\n        "type": "function",\n        "function": {\n            "name": tool.name,\n            "description": tool.description,\n            "type": "function",\n            "parameters": {\n                "type": "object",\n                "properties": tool.inputSchema["properties"]\n            }\n        }\n    }\n\n    return tool_schema\n\n'})}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"call_llm"})," will help us call an LLM (this calls GitHub Models so if you're in GitHub Codespaces this will just work, if not, you need to set up a PAT, personal access token)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"convert_to_llm_tool"}),", this function will be called after a first initial call to the server where we ask for its tools. For each tool from the MCP server, we will convert them to a format the LLM will understand."]}),"\n"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Let's define our ",(0,s.jsx)(n.code,{children:"run"})," function next:"]}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def run():\n  async with stdio_client(server_params) as (read, write):\n      async with ClientSession(\n          read, write\n      ) as session:\n          # Initialize the connection\n          await session.initialize()\n\n          # List available resources\n          resources = await session.list_resources()\n          print("LISTING RESOURCES")\n          for resource in resources:\n              print("Resource: ", resource)\n\n          # 1. List available tools\n          tools = await session.list_tools()\n          print("LISTING TOOLS")\n\n          functions = []\n\n          # 2. convert tools to LLM tool format\n          for tool in tools.tools:\n              print("Tool: ", tool.name)\n              print("Tool", tool.inputSchema["properties"])\n              functions.append(convert_to_llm_tool(tool))\n          \n          prompt = "Add 2 to 20"\n          # 3. ask LLM what tools to all, if any\n          functions_to_call = call_llm(prompt, functions)\n\n          # 4. call suggested functions\n          for f in functions_to_call:\n              result = await session.call_tool(f["name"], arguments=f["args"])\n              print("TOOLS result: ", result.content)\n\n\nif __name__ == "__main__":\n    import asyncio\n\n    asyncio.run(run())\n'})}),(0,s.jsx)(n.p,{children:"Here's what happens:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"First, we ask the server for its tools."}),"\n",(0,s.jsx)(n.li,{children:"For each of its tools, we convert it to an LLM type tool."}),"\n",(0,s.jsx)(n.li,{children:"Now we call the LLM with a prompt and set of tools we just converted."}),"\n",(0,s.jsx)(n.li,{children:"Next, we call suggsted tools and print the results"}),"\n"]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"-2--run-the-client",children:"-2- Run the client"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(a.A,{value:"typescript",label:"TypeScript",children:[(0,s.jsxs)(n.p,{children:["To test this client out, you're recommended to add a task to your ",(0,s.jsx)(n.em,{children:"package.json"}),' called "client" that runs the client. This way you can run the client with ',(0,s.jsx)(n.code,{children:"npm run client"}),"."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "scripts": {\n    "client": "tsx node build/client.js"\n  }\n}\n'})}),(0,s.jsx)(n.p,{children:"You should see the following output:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Asking server for available tools\nQuerying LLM:  What is the sum of 2 and 3?\nMaking tool call\nTool result:  { content: [ { type: 'text', text: '5' } ] }\n"})})]}),(0,s.jsxs)(a.A,{value:"python",label:"Python",default:!0,children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"python client.py\n"})}),(0,s.jsx)(n.p,{children:"You should see a response similar to:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Asking server for available tools\nQuerying LLM:  What is the sum of 2 and 3?\nMaking tool call\nTool result:  { content: [ { type: 'text', text: '5' } ] }\n"})})]})]}),"\n",(0,s.jsx)(n.p,{children:"Here's a repository you can clone if you want to test this out:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/softchris/tutorial-mcp.git\ncd tutorial-mcp\nnpm run client\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"You've learned to integrate LLM as part of your client. The LLM is now the interface between the user and the server. You can either add the client code to your existing project or you can clone the repository below to see a working solution:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/softchris/mcp-workshop.git\ncd mcp-workshop\n"})})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},5537:(e,n,t)=>{t.d(n,{A:()=>b});var o=t(6540),s=t(4164),l=t(5627),r=t(6347),a=t(372),i=t(604),c=t(1861),d=t(8749);function h(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:s}}=e;return{value:n,label:t,attributes:o,default:s}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const s=(0,r.W6)(),l=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,i.aZ)(l),(0,o.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(s.location.search);n.set(l,e),s.replace({...s.location,search:n.toString()})}),[l,s])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,l=u(e),[r,i]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:l}))),[c,h]=m({queryString:t,groupId:s}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[s,l]=(0,d.Dv)(t);return[s,(0,o.useCallback)((e=>{t&&l.set(e)}),[t,l])]}({groupId:s}),x=(()=>{const e=c??f;return p({value:e,tabValues:l})?e:null})();(0,a.A)((()=>{x&&i(x)}),[x]);return{selectedValue:r,selectValue:(0,o.useCallback)((e=>{if(!p({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),h(e),g(e)}),[h,g,l]),tabValues:l}}var g=t(9136);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=t(4848);function v(e){let{className:n,block:t,selectedValue:o,selectValue:r,tabValues:a}=e;const i=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.a_)(),d=e=>{const n=e.currentTarget,t=i.indexOf(n),s=a[t].value;s!==o&&(c(n),r(s))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1];break}}n?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:a.map((e=>{let{value:n,label:t,attributes:l}=e;return(0,j.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>{i.push(e)},onKeyDown:h,onClick:d,...l,className:(0,s.A)("tabs__item",x.tabItem,l?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function y(e){let{lazy:n,children:t,selectedValue:l}=e;const r=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=r.find((e=>e.props.value===l));return e?(0,o.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:r.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==l})))})}function w(e){const n=f(e);return(0,j.jsxs)("div",{className:(0,s.A)("tabs-container",x.tabList),children:[(0,j.jsx)(v,{...n,...e}),(0,j.jsx)(y,{...n,...e})]})}function b(e){const n=(0,g.A)();return(0,j.jsx)(w,{...e,children:h(e.children)},String(n))}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(6540);const s={},l=o.createContext(s);function r(e){const n=o.useContext(l);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(l.Provider,{value:n},e.children)}},9329:(e,n,t)=>{t.d(n,{A:()=>r});t(6540);var o=t(4164);const s={tabItem:"tabItem_Ymn6"};var l=t(4848);function r(e){let{children:n,hidden:t,className:r}=e;return(0,l.jsx)("div",{role:"tabpanel",className:(0,o.A)(s.tabItem,r),hidden:t,children:n})}}}]);